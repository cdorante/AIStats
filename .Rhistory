#Ex
Mx = X.mean()
Mx
Mx.shape
Mx2= np.outer(Mx,Mx.transpose())
Mx2
COV = Ex2 - n/(n-1) * Mx2
COV
cov1=X.cov()
type(cov1)
cov1
SD = np.sqrt(np.diag(COV))
SD
# i can check whether the calculated standard deviations are correct:
X.std()
D =np.diag(SD)
D
CORR = np.linalg.inv(D) @ COV @ np.linalg.inv(D)
CORR
corr1 = X.corr()
corr1
View(XY2022c)
XY.drop(axis=1,columns=['epspw'],inplace=True)
View(XY)
model1 = sm.OLS.from_formula('f1ry ~ .,data=XYc,missing='drop').fit()
model1 = sm.OLS.from_formula('f1ry ~ ,data=XYc,missing='drop').fit()
model1 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large',data=XYc,missing='drop').fit()
model1 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large',data=XYc,missing='drop').fit()
print(model1.summary())
model2 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large + sizeg_medium:oepspw + sizeg_large:oepspw',data=XYc,missing='drop').fit()
print(model2.summary())
View(XYc)
View(XY)
View(XYc)
View(XY)
XY.columns
XYc=sm.add_constant(XY)
View(XYc)
View(X)
View(XY)
Xc = XY.drop(columns=['f1ry'],axis=1)
View(X)
Xc = XYc.drop(columns=['f1ry'],axis=1)
View(Xc)
Y1 = XY['f1y'].to_frame()
Y1 = XY["f1ry"].to_frame()
Y = XY["f1ry"].to_frame()
xtx = np.matmul(X.transpose(),X)
Xc = XYc.drop(columns=['f1ry'],axis=1).squeeze()
type(Xc)
Y = XY["f1ry"]
type(Y)
xtx = np.matmul(X.transpose(),X)
type(Xc)
Xc = XYc.drop(columns=['f1ry'],axis=1)
type(Xc)
Y = XY["f1ry"].to_frame()
Y = XY["f1ry"].copy()
Y = Y.squeeze()
type(Y)
Xc = Xc.squeeze()
type(Xc)
Xc = XYc.drop(columns=['f1ry'],axis=1)
Y = XY["f1ry"].to_frame()
View(XYc)
xtx = np.matmul(X.transpose(),X)
xty = np.matmul(X.transpose(),Y)
xtx = np.matmul(Xc.transpose(),Xc)
xty = np.matmul(Xc.transpose(),Y)
invtxt = np.linalg.inv(xtx)
betas = np.matmul(invtxt,xty)
betas
xtx = Xc.transpose @ Xc
xtx = Xc.transpose() @ Xc
View(xtx)
View(Xc)
Xc = Xc.dropna()
XYc = XYc.dropna()
Xc = XYc.drop(columns=['f1ry'],axis=1)
Y = XYc["f1ry"].to_frame()
xtx = Xc.transpose() @ Xc
xtx2 = np.matmul(Xc.transpose(),Xc)
View(xtx)
View(xtx2)
View(xtx)
xty = Xc.transpose() @ Y
invtxt = np.linalg.inv(xtx)
betas = invtxt @ xtx
betas
xtx = Xc.transpose() @ Xc
xty = Xc.transpose() @ Y
invtxt = np.linalg.inv(xtx)
betas = invtxt @ xtx
betas
Xc = Xc.dropna().values
Y = XYc["f1ry"].values
xtx = Xc.transpose() @ Xc
xty = Xc.transpose() @ Y
invtxt = np.linalg.inv(xtx)
betas = invtxt @ xtx
betas
xtx = np.matmul(Xc.transpose(),Xc)
xty = np.matmul(Xc.transpose(),Y)
invtxt = np.linalg.inv(xtx)
betas = np.matmul(invtxt,xty)
betas
xtx = Xc.transpose() @ Xc
xty = Xc.transpose() @ Y
invtxt = np.linalg.inv(xtx)
betas = invtxt @ xty
betas
xtx = np.matmul(Xc.transpose(),Xc)
xty = np.matmul(Xc.transpose(),Y)
invtxt = np.linalg.inv(xtx)
invtxt = np.linalg.inv(xtx)
betas = np.matmul(invtxt,xty)
betas
Xc = Xc.dropna()
Y = XYc["f1ry"]
Xc = XYc.drop(columns=['f1ry'],axis=1)
Xc = Xc.dropna()
Y = XYc["f1ry"]
xtx = Xc.transpose() @ Xc
xty = Xc.transpose() @ Y
invtxt = np.linalg.inv(xtx)
invtxt = np.linalg.inv(xtx)
betas = invtxt @ xty
betas
Y = XYc["f1ry"].to_frame()
xty = Xc.transpose() @ Y
betas = invtxt @ xty
betas = invtxt @ xty
betas
XY2022 = XY2022[["firm","f1ry","oepspw","bmrw","revgrowthw","egrowthw","pmw","atow","sizeg_medium","sizeg_large"]]
XY2022 = XY2022[["firm","f1ry","oepspw","bmrw","revgrowthw","egrowthw","pmw","atow","sizeg_medium","sizeg_large"]]
# I set firm as the index to identify each row:
XY2022.set_index(['firm'], inplace=True)
```{python}
```{python}
XY2022 = dataind[dataind['yearf']==2022].copy()
XY2022 = dataind[dataind['yearf']==2022].copy()
# I reset the index to get the firm as column:
XY2022.reset_index(inplace=True)
XY2022.reset_index(inplace=True)
XY2022 = XY2022[["firm","f1ry","oepspw","bmrw","revgrowthw","egrowthw","pmw","atow","sizeg_medium","sizeg_large"]]
# I set firm as the index to identify each row:
XY2022.set_index(['firm'], inplace=True)
XY2022c = sm.add_constant(XY2022)
model3 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large + sizeg_medium:oepspw + sizeg_large:oepspw',data=XY2022c,missing='drop').fit()
model3 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large + sizeg_medium:oepspw + sizeg_large:oepspw',data=XY2022c,missing='drop').fit()
print(model3.summary())
X=XY2022[["oepspw","bmrw","revgrowthw","egrowthw","pmw","atow","sizeg_medium","sizeg_large"]]
Y = XY2022["f1ry"].to_frame()
Xc = sm.add_constant(X).dropna()
Xc = XY2022.drop(columns=['f1ry'],axis=1).dropna()
Xc=sm.add_constant(Xc)
View(Xc)
XY2022=XY2022.dropna()
Xc = XY2022.drop(columns=['f1ry'],axis=1)
Xc = XY2022.drop(columns=['f1ry'],axis=1)
Xc=sm.add_constant(Xc)
Y = XY2022["f1ry"].to_frame()
xtx = Xc.transpose() @ Xc
xty = Xc.transpose() @ Y
invtxt = np.linalg.inv(xtx)
betas = invtxt @ xty
betas
Yhat = Xc @ betas
View(Xc)
View(betas)
betas.shape
Yhat = Xc @ betas.transpose()
Xc.shape
betas.shape
Yhat = Xc @ betas
Yhat = np.matmul(Xc, betas)
View(Y)
Yhat = Xc @ betas
Yhat = Xc @ betas.transpose()
Yhat = Xc.transpose() @ betas
Yhat = np.matmul(Xc, betas)
View(Yhat)
View(Y)
errors = Yhat - Y
View(errors)
Yhat.columns=['Yhat']
View(Yhat)
Yhat['Y'] = Y['f1yr']
View(Y)
Yhat['Y'] = Y['f1ry']
View(Yhat)
Yhat['errors'] = Yhat.Y - Yhat.Yat
Yhat['errors'] = Yhat.Y - Yhat.Yhat
View(Yhat)
Yhat['errors'].std()
Yhat['errors'].var()
stderrbetas = Yhat['errors'].var() * invtxt
stderrbetas = stderrbetas.diagonal()
stderrbetas
stderrbetas = stderrbetas.diagonal().transpose()
stderrbetas = stderrbetas.diagonal().transpose()
stderrbetas
stderrbetas = Yhat['errors'].var() * invtxt
stderrbetas = Yhat['errors'].var() * invtxt
stderrbetas = stderrbetas.diagonal().transpose()
stderrbetas
stderrbetas = Yhat['errors'].var() * invtxt
stderrbetas = stderrbetas.diagonal()
stderrbetas
model3 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large + sizeg_medium:oepspw + sizeg_large:oepspw',data=XY2022c,missing='drop').fit()
model3 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large + sizeg_medium:oepspw + sizeg_large:oepspw',data=XY2022c,missing='drop').fit()
print(model3.summary())
model3.predict()
View(Yhat)
model2 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large + sizeg_medium:oepspw + sizeg_large:oepspw',data=XYc,missing='drop').fit()
print(model2.summary())
model2 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large + sizeg_medium:oepspw + sizeg_large:oepspw',data=XY2022,missing='drop').fit()
model2 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large + sizeg_medium:oepspw + sizeg_large:oepspw',data=XY2022,missing='drop').fit()
print(model2.summary())
model2 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large + sizeg_medium:oepspw ,data=XY2022,missing='drop').fit()
print(model2.summary())
model2 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large  ,data=XY2022,missing='drop').fit()
print(model2.summary())
model2 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large  ,data=XY2022c,missing='drop').fit()
print(model2.summary())
model2 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large', data=XY2022c,missing='drop').fit()
print(model2.summary())
Yhat = np.dot(Xc, betas)
model2.predict
model2.predict()
Yhat = np.dot(Xc, betas)
Yhat.columns=['Yhat']
Yhat['Y'] = Y['f1ry']
Yhat['errors'] = Yhat.Y - Yhat.Yhat
Yhat['errors'].std()
# The standard errors of the beta coefficients are:
stderrbetas = Yhat['errors'].var() * invtxt
stderrbetas = stderrbetas.diagonal()
stderrbetas
Yhat = np.dot(Xc, betas)
Yhat.columns=['Yhat']
Yhat = np.dot(Xc, betas)
Yhat = np.mamult(Xc, betas)
Yhat = np.mamul(Xc, betas)
Yhat = np.matmul(Xc, betas)
Yhat.columns=['Yhat']
Yhat['Y'] = Y['f1ry']
Yhat['errors'] = Yhat.Y - Yhat.Yhat
Yhat['errors'] = Yhat.Y - Yhat.Yhat
Yhat['errors'].std()
stderrbetas = Yhat['errors'].var() * invtxt
stderrbetas = Yhat['errors'].var() * invtxt
stderrbetas = stderrbetas.diagonal()
stderrbetas = stderrbetas.diagonal()
stderrbetas
# Using matrix algebra to estimate the beta coefficients:
XY2022=XY2022.dropna()
Xc = XY2022.drop(columns=['f1ry'],axis=1)
Xc=sm.add_constant(Xc)
Y = XY2022["f1ry"].to_frame()
xtx = Xc.transpose() @ Xc
xty = Xc.transpose() @ Y
invtxt = np.linalg.inv(xtx)
betas = invtxt @ xty
betas
Yhat = np.matmul(Xc, betas)
Yhat['Y'] = Y['f1ry']
Yhat['errors'] = Yhat.Y - Yhat.Yhat
Yhat = np.matmul(Xc, betas)
View(Yhat)
Yhat.columns=['Yhat']
View(Yhat)
Yhat['Y'] = Y['f1ry']
Yhat['errors'] = Yhat.Y - Yhat.Yhat
Yhat['errors'] = Yhat.Y - Yhat.Yhat
Yhat['errors'].std()
# The standard errors of the beta coefficients are:
# The standard errors of the beta coefficients are:
stderrbetas = Yhat['errors'].var() * invtxt
# The standard errors of the beta coefficients are:
stderrbetas = Yhat['errors'].var() * invtxt
stderrbetas = stderrbetas.diagonal()
stderrbetas
stderrbetas = stderrbetas.diagonal().sqrt()
stderrbetas = Yhat['errors'].var() * invtxt
stderrbetas = stderrbetas.diagonal().sqrt()
stderrbetas = np.sqrt(stderrbetas.diagonal())
stderrbetas
(Yhat['Y'] - Yhat['Y'].mean())**2.sum()
pow(Yhat['Y'] - Yhat['Y'].mean(),2).sum()
SSR = pow(Yhat['Yhat'] - Yhat['Y'].mean(),2).sum
SST=pow(Yhat['Y'] - Yhat['Y'].mean(),2).sum
SSR = pow(Yhat['Yhat'] - Yhat['Y'].mean(),2).sum()
SST=pow(Yhat['Y'] - Yhat['Y'].mean(),2).sum()
SSE = pow(Yhat['Yhat'] - Yhat['Y'],2).sum()
R2 = SSR / SST
R2 = SSR / SST
R2
H = np.dot(Xc,np.dot(np.linalg.inv(np.dot(Xc.transpose(),Xc)),Xc.transpose()))
H.shape
Yhat1 = np.dot(H,Y1)
Yhat1 = np.dot(H,Y1)
Yhat1
Yhat1 = np.dot(H,Y1)
Yhat1 = np.dot(H,Y)
Yhat1
h = np.diag(H)
p = Xc.shape[1] - 1
p = Xc.shape[1] - 1
cutoffh = 3*Xc.shape[1] / Xc.shape[0]
hdf = pd.DataFrame(h,index=Xc.index,columns=["h"])
hdf["leverage"]=np.where(hdf['h']>cutoffh,1,0)
hdf.leverage.sum()
hdf[hdf['leverage']==1]
hdf.leverage.sum()
hdf[hdf['leverage']==1]
Yhat1 = np.dot(H,Y)
Yhat1
# The beta coefficients are estimated as:
b = np.dot(np.linalg.inv(np.dot(X1.transpose(),X1)),np.dot(X1.transpose(),Y1))
b
#I can calculate the predicted values using the b coefficient vector:
Yhat2 = np.dot(X1,b)
Yhat2
# I calculate the predicted values with the predict function of the regression object:
Yhat3 = model2.predict()
Yhat3
View(betas)
b
Yhat1 = np.dot(H,Y)
Yhat1
# The beta coefficients are estimated as:
b = np.dot(np.linalg.inv(np.dot(Xc.transpose(),Xc)),np.dot(Xc.transpose(),Y))
b
#I can calculate the predicted values using the b coefficient vector:
Yhat2 = np.dot(Xc,b)
Yhat2
# I calculate the predicted values with the predict function of the regression object:
Yhat3 = model2.predict()
Yhat3
b = np.dot(np.linalg.inv(np.dot(Xc.transpose(),Xc)),np.dot(Xc.transpose(),Y))
b
betas
types(b)
type(b)
type(betas)
betas = np.linalg.inv(Xc.transpose() @ Xc) @ (Xc.transpose() @ Y )
betas
h = np.diag(H)
#p is the # of independent variables; X1 has the constant, so the # of X1 columns
#   will be equal to p+1
p = Xc.shape[1] - 1
cutoffh = 3*Xc.shape[1] / Xc.shape[0]
# I create the dataframe with the index equal to the stock tickers:
hdf = pd.DataFrame(h,index=Xc.index,columns=["h"])
# I create the binary variable to identify possible influential points:
hdf["leverage"]=np.where(hdf['h']>cutoffh,1,0)
# I see  how many possible influential points there are:
hdf.leverage.sum()
hdf[hdf['leverage']==1]
import statsmodels.tools.eval_measures as eval
# I calculate the errors for each observation:
errors = Y1 - Yhat3
# I calculate the squared errors for each observation:
errorssq = np.square(errors)
# I calculate the sum of squared errors:
SSE = errorssq.sum()
# I calculate the mean squared errors:
MSE = SSE / (n)
MSE1 = SSE / (n-p+1)
# i calculate MSE using a function from statsmodels:
mse = eval.mse(Y1,Yhat)
rmse = eval.rmse(Y1,Yhat)
# Note that statsmodels calculates MSE using as denominator n instead of (n-p+1)!
#    Which is correct? or more adequate?
import statsmodels.tools.eval_measures as eval
# I calculate the errors for each observation:
errors = Y1 - Yhat3
# I calculate the squared errors for each observation:
errorssq = np.square(errors)
# I calculate the sum of squared errors:
SSE = errorssq.sum()
# I calculate the mean squared errors:
MSE = SSE / (n)
MSE1 = SSE / (n-p+1)
# i calculate MSE using a function from statsmodels:
mse = eval.mse(Y,Yhat)
rmse = eval.rmse(Y,Yhat)
# Note that statsmodels calculates MSE using as denominator n instead of (n-p+1)!
# Which is correct? or more adequate?
errors = Y1 - Yhat3
errors = Y - Yhat3
Y
Yhat3
errors = Y - Yhat['Yhat']
errorssq = np.square(errors)
SSE = errorssq.sum()
type(errorssq)
errorssq
errors = Y - Yhat['Yhat']
errors
Y
type(Y)
type(Yhat)
type(Yhat['Yhat'])
View(Yhat)
errorssq = np.square(Yhat['errors'])
errorssq = np.square(Yhat['errors']).to_frame()
SSE = errorssq.sum()
SSE
MSE = SSE / (n)
MSE1 = SSE / (n-p+1)
mse = eval.mse(Y,Yhat)
rmse = eval.rmse(Y,Yhat)
mse
MSE
rmse
mse = eval.mse(Y,Yhat['Yhat'])
mse
mse = eval.mse(Y,Yhat2)
rmse = eval.rmse(Y,Yhat2)
mse
rmse
MSE
hdf['error'] = Yhat['errors']
hdf["stres"]= hdf['error']/np.sqrt((MSE1*(1-hdf['h'])))
hdf['stres2'] = hdf['error']/np.sqrt(MSE1*(n-1)/n)
hdf
View(hdf)
hdf['error'].values
hdf['stres']= hdf['error'].values / np.sqrt((MSE1*(1-hdf['h'])))
hdf['stres']= hdf['error'].values / np.sqrt((MSE1*(1-hdf['h'].values)))
hdf['stres']= hdf['error'] / np.sqrt((mse*(1-hdf['h'])))
hdf['stres2'] = hdf['error']/np.sqrt(mse*(n-1)/n)
hdf
hdf[np.abs(hdf['stres'])>3].count()
hdf[(hdf['leverage']==1) & (np.abs(hdf['stres'])>3)].count()
hdf[(hdf['leverage']==1) & (np.abs(hdf['stres'])>3)]
hdf['delres']= hdf['stres']* np.sqrt((n-p-2)/(n-p-1-np.square(hdf['stres'])))
hdf[np.abs(hdf['delres'])>3].count
hdf[(hdf['leverage']==1) & (np.abs(hdf['delres'])>3)]
from statsmodels.stats.outliers_influence import OLSInfluence
influence=OLSInfluence(model2)
influence.resid_std
influence.resid_studentized
influence.hat_matrix_diag
hdf
influence.dffits
hdf['dffits']=influence.dffits[0]
hdf
cutoff_dffit =(2*np.sqrt((p+2)/(n-p-2)))
hdf[np.abs(hdf['dffits'])> cutoff_dffit]
hdf[(hdf['leverage']==1) & (np.abs(hdf['dffits'])> cutoff_dffit)]
hdf['cookd']=influence.cooks_distance[0]
hdf
hdf[hdf['cookd']>1]
hdf[hdf['cookd']>(4/n)]
hdf[(hdf['leverage']==1) & (np.abs(hdf['dffits'])> cutoff_dffit) & hdf['cookd']>(4/n)]
hdf[(hdf['leverage']==1) & (np.abs(hdf['dffits'])> cutoff_dffit) & hdf['cookd']>(4/n)].count()
tobedeleted = hdf[np.abs(hdf['delres'])>3]
tobedeleted.index
X1 = Xc.merge(tobedeleted, how='left', left_index=True, right_index=True, indicator=True)
View(X1)
View(Xc)
Xc.columns
X1=X1.query("_merge == 'left_only'")[['const', 'oepspw', 'bmrw', 'revgrowthw', 'egrowthw', 'pmw', 'atow','sizeg_medium', 'sizeg_large']]
X1.columns
X1.shape
Xc.shape
X1.shape
X1
Y1 = tobedeleted.merge(Y, how='right', left_index=True, right_index=True, indicator=True)
Y1=Y1.query("_merge == 'right_only'")['f1ry']
Y1=Y1.query("_merge == 'right_only'")['f1ry'].to_frame()
Y1.shape
Y1
X1.shape
model4 = sm.OLS(Y1,X1,missing='drop').fit()
print(model4.summary())
print(model2.summary())
reticulate::repl_python()
import pandas as pd
import numpy as np
data = pd.read_csv("dataus2023.csv")
data.shape
firms = pd.read_csv("firmsus2023.csv")
firms.shape
firms.columns
# I keep only the columns I need: company code, company name, status and industry:
firms1 = firms[["empresa","Nombre","status","naics1"]]
firms1.columns=['firm','Empresa','status','industria']
# I do a left join using the panel data as the left dataset:
data = pd.merge(data, firms1, on="firm", how='left')
data['qdate'] = pd.PeriodIndex(data.q, freq="Q")
data.set_index(['firm','qdate'], inplace=True)
data.head()
data.index
# Annual log returns:
data['ry'] = np.log(data['adjprice']) - np.log(data.groupby(['firm'])['adjprice'].shift(4))
# Quarterly log returns:
data['rq'] = np.log(data['adjprice']) - np.log(data.groupby(['firm'])['adjprice'].shift(1))
# Note that I use groupby function to ensure that the returns of the first quarters of a firm are not calculated since the previous row belongs to other firm!
# Future quarterly and annual returns (1 quarter in the future):
data['f1rq'] = data.groupby(['firm'])['rq'].shift(-1)
data['f4rq'] = data.groupby(['firm'])['rq'].shift(-4)
data['f1ry'] = data.groupby(['firm'])['ry'].shift(-1)
data['f4ry'] = data.groupby(['firm'])['ry'].shift(-4)
# I will use the future returns as dependent variable, so I will be able to understand which current variables might be statistically related to future returns
# I check whether the return calculations are correct. I select rows where I can see 2 different firms and check whether returns and future returns where calculated correctly:
data[["fiscalmonth","adjprice","rq","ry","f1ry","f4ry"]].iloc[182:210]
quit
load("C:/ATec/202313/AIStats/.RData")
