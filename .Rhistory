model2 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large  ,data=XY2022c,missing='drop').fit()
print(model2.summary())
model2 = sm.OLS.from_formula('f1ry ~ oepspw + bmrw + revgrowthw + egrowthw + pmw + atow + sizeg_medium + sizeg_large', data=XY2022c,missing='drop').fit()
print(model2.summary())
Yhat = np.dot(Xc, betas)
model2.predict
model2.predict()
Yhat = np.dot(Xc, betas)
Yhat.columns=['Yhat']
Yhat['Y'] = Y['f1ry']
Yhat['errors'] = Yhat.Y - Yhat.Yhat
Yhat['errors'].std()
# The standard errors of the beta coefficients are:
stderrbetas = Yhat['errors'].var() * invtxt
stderrbetas = stderrbetas.diagonal()
stderrbetas
Yhat = np.dot(Xc, betas)
Yhat.columns=['Yhat']
Yhat = np.dot(Xc, betas)
Yhat = np.mamult(Xc, betas)
Yhat = np.mamul(Xc, betas)
Yhat = np.matmul(Xc, betas)
Yhat.columns=['Yhat']
Yhat['Y'] = Y['f1ry']
Yhat['errors'] = Yhat.Y - Yhat.Yhat
Yhat['errors'] = Yhat.Y - Yhat.Yhat
Yhat['errors'].std()
stderrbetas = Yhat['errors'].var() * invtxt
stderrbetas = Yhat['errors'].var() * invtxt
stderrbetas = stderrbetas.diagonal()
stderrbetas = stderrbetas.diagonal()
stderrbetas
# Using matrix algebra to estimate the beta coefficients:
XY2022=XY2022.dropna()
Xc = XY2022.drop(columns=['f1ry'],axis=1)
Xc=sm.add_constant(Xc)
Y = XY2022["f1ry"].to_frame()
xtx = Xc.transpose() @ Xc
xty = Xc.transpose() @ Y
invtxt = np.linalg.inv(xtx)
betas = invtxt @ xty
betas
Yhat = np.matmul(Xc, betas)
Yhat['Y'] = Y['f1ry']
Yhat['errors'] = Yhat.Y - Yhat.Yhat
Yhat = np.matmul(Xc, betas)
View(Yhat)
Yhat.columns=['Yhat']
View(Yhat)
Yhat['Y'] = Y['f1ry']
Yhat['errors'] = Yhat.Y - Yhat.Yhat
Yhat['errors'] = Yhat.Y - Yhat.Yhat
Yhat['errors'].std()
# The standard errors of the beta coefficients are:
# The standard errors of the beta coefficients are:
stderrbetas = Yhat['errors'].var() * invtxt
# The standard errors of the beta coefficients are:
stderrbetas = Yhat['errors'].var() * invtxt
stderrbetas = stderrbetas.diagonal()
stderrbetas
stderrbetas = stderrbetas.diagonal().sqrt()
stderrbetas = Yhat['errors'].var() * invtxt
stderrbetas = stderrbetas.diagonal().sqrt()
stderrbetas = np.sqrt(stderrbetas.diagonal())
stderrbetas
(Yhat['Y'] - Yhat['Y'].mean())**2.sum()
pow(Yhat['Y'] - Yhat['Y'].mean(),2).sum()
SSR = pow(Yhat['Yhat'] - Yhat['Y'].mean(),2).sum
SST=pow(Yhat['Y'] - Yhat['Y'].mean(),2).sum
SSR = pow(Yhat['Yhat'] - Yhat['Y'].mean(),2).sum()
SST=pow(Yhat['Y'] - Yhat['Y'].mean(),2).sum()
SSE = pow(Yhat['Yhat'] - Yhat['Y'],2).sum()
R2 = SSR / SST
R2 = SSR / SST
R2
H = np.dot(Xc,np.dot(np.linalg.inv(np.dot(Xc.transpose(),Xc)),Xc.transpose()))
H.shape
Yhat1 = np.dot(H,Y1)
Yhat1 = np.dot(H,Y1)
Yhat1
Yhat1 = np.dot(H,Y1)
Yhat1 = np.dot(H,Y)
Yhat1
h = np.diag(H)
p = Xc.shape[1] - 1
p = Xc.shape[1] - 1
cutoffh = 3*Xc.shape[1] / Xc.shape[0]
hdf = pd.DataFrame(h,index=Xc.index,columns=["h"])
hdf["leverage"]=np.where(hdf['h']>cutoffh,1,0)
hdf.leverage.sum()
hdf[hdf['leverage']==1]
hdf.leverage.sum()
hdf[hdf['leverage']==1]
Yhat1 = np.dot(H,Y)
Yhat1
# The beta coefficients are estimated as:
b = np.dot(np.linalg.inv(np.dot(X1.transpose(),X1)),np.dot(X1.transpose(),Y1))
b
#I can calculate the predicted values using the b coefficient vector:
Yhat2 = np.dot(X1,b)
Yhat2
# I calculate the predicted values with the predict function of the regression object:
Yhat3 = model2.predict()
Yhat3
View(betas)
b
Yhat1 = np.dot(H,Y)
Yhat1
# The beta coefficients are estimated as:
b = np.dot(np.linalg.inv(np.dot(Xc.transpose(),Xc)),np.dot(Xc.transpose(),Y))
b
#I can calculate the predicted values using the b coefficient vector:
Yhat2 = np.dot(Xc,b)
Yhat2
# I calculate the predicted values with the predict function of the regression object:
Yhat3 = model2.predict()
Yhat3
b = np.dot(np.linalg.inv(np.dot(Xc.transpose(),Xc)),np.dot(Xc.transpose(),Y))
b
betas
types(b)
type(b)
type(betas)
betas = np.linalg.inv(Xc.transpose() @ Xc) @ (Xc.transpose() @ Y )
betas
h = np.diag(H)
#p is the # of independent variables; X1 has the constant, so the # of X1 columns
#   will be equal to p+1
p = Xc.shape[1] - 1
cutoffh = 3*Xc.shape[1] / Xc.shape[0]
# I create the dataframe with the index equal to the stock tickers:
hdf = pd.DataFrame(h,index=Xc.index,columns=["h"])
# I create the binary variable to identify possible influential points:
hdf["leverage"]=np.where(hdf['h']>cutoffh,1,0)
# I see  how many possible influential points there are:
hdf.leverage.sum()
hdf[hdf['leverage']==1]
import statsmodels.tools.eval_measures as eval
# I calculate the errors for each observation:
errors = Y1 - Yhat3
# I calculate the squared errors for each observation:
errorssq = np.square(errors)
# I calculate the sum of squared errors:
SSE = errorssq.sum()
# I calculate the mean squared errors:
MSE = SSE / (n)
MSE1 = SSE / (n-p+1)
# i calculate MSE using a function from statsmodels:
mse = eval.mse(Y1,Yhat)
rmse = eval.rmse(Y1,Yhat)
# Note that statsmodels calculates MSE using as denominator n instead of (n-p+1)!
#    Which is correct? or more adequate?
import statsmodels.tools.eval_measures as eval
# I calculate the errors for each observation:
errors = Y1 - Yhat3
# I calculate the squared errors for each observation:
errorssq = np.square(errors)
# I calculate the sum of squared errors:
SSE = errorssq.sum()
# I calculate the mean squared errors:
MSE = SSE / (n)
MSE1 = SSE / (n-p+1)
# i calculate MSE using a function from statsmodels:
mse = eval.mse(Y,Yhat)
rmse = eval.rmse(Y,Yhat)
# Note that statsmodels calculates MSE using as denominator n instead of (n-p+1)!
# Which is correct? or more adequate?
errors = Y1 - Yhat3
errors = Y - Yhat3
Y
Yhat3
errors = Y - Yhat['Yhat']
errorssq = np.square(errors)
SSE = errorssq.sum()
type(errorssq)
errorssq
errors = Y - Yhat['Yhat']
errors
Y
type(Y)
type(Yhat)
type(Yhat['Yhat'])
View(Yhat)
errorssq = np.square(Yhat['errors'])
errorssq = np.square(Yhat['errors']).to_frame()
SSE = errorssq.sum()
SSE
MSE = SSE / (n)
MSE1 = SSE / (n-p+1)
mse = eval.mse(Y,Yhat)
rmse = eval.rmse(Y,Yhat)
mse
MSE
rmse
mse = eval.mse(Y,Yhat['Yhat'])
mse
mse = eval.mse(Y,Yhat2)
rmse = eval.rmse(Y,Yhat2)
mse
rmse
MSE
hdf['error'] = Yhat['errors']
hdf["stres"]= hdf['error']/np.sqrt((MSE1*(1-hdf['h'])))
hdf['stres2'] = hdf['error']/np.sqrt(MSE1*(n-1)/n)
hdf
View(hdf)
hdf['error'].values
hdf['stres']= hdf['error'].values / np.sqrt((MSE1*(1-hdf['h'])))
hdf['stres']= hdf['error'].values / np.sqrt((MSE1*(1-hdf['h'].values)))
hdf['stres']= hdf['error'] / np.sqrt((mse*(1-hdf['h'])))
hdf['stres2'] = hdf['error']/np.sqrt(mse*(n-1)/n)
hdf
hdf[np.abs(hdf['stres'])>3].count()
hdf[(hdf['leverage']==1) & (np.abs(hdf['stres'])>3)].count()
hdf[(hdf['leverage']==1) & (np.abs(hdf['stres'])>3)]
hdf['delres']= hdf['stres']* np.sqrt((n-p-2)/(n-p-1-np.square(hdf['stres'])))
hdf[np.abs(hdf['delres'])>3].count
hdf[(hdf['leverage']==1) & (np.abs(hdf['delres'])>3)]
from statsmodels.stats.outliers_influence import OLSInfluence
influence=OLSInfluence(model2)
influence.resid_std
influence.resid_studentized
influence.hat_matrix_diag
hdf
influence.dffits
hdf['dffits']=influence.dffits[0]
hdf
cutoff_dffit =(2*np.sqrt((p+2)/(n-p-2)))
hdf[np.abs(hdf['dffits'])> cutoff_dffit]
hdf[(hdf['leverage']==1) & (np.abs(hdf['dffits'])> cutoff_dffit)]
hdf['cookd']=influence.cooks_distance[0]
hdf
hdf[hdf['cookd']>1]
hdf[hdf['cookd']>(4/n)]
hdf[(hdf['leverage']==1) & (np.abs(hdf['dffits'])> cutoff_dffit) & hdf['cookd']>(4/n)]
hdf[(hdf['leverage']==1) & (np.abs(hdf['dffits'])> cutoff_dffit) & hdf['cookd']>(4/n)].count()
tobedeleted = hdf[np.abs(hdf['delres'])>3]
tobedeleted.index
X1 = Xc.merge(tobedeleted, how='left', left_index=True, right_index=True, indicator=True)
View(X1)
View(Xc)
Xc.columns
X1=X1.query("_merge == 'left_only'")[['const', 'oepspw', 'bmrw', 'revgrowthw', 'egrowthw', 'pmw', 'atow','sizeg_medium', 'sizeg_large']]
X1.columns
X1.shape
Xc.shape
X1.shape
X1
Y1 = tobedeleted.merge(Y, how='right', left_index=True, right_index=True, indicator=True)
Y1=Y1.query("_merge == 'right_only'")['f1ry']
Y1=Y1.query("_merge == 'right_only'")['f1ry'].to_frame()
Y1.shape
Y1
X1.shape
model4 = sm.OLS(Y1,X1,missing='drop').fit()
print(model4.summary())
print(model2.summary())
reticulate::repl_python()
import pandas as pd
import numpy as np
data = pd.read_csv("dataus2023.csv")
data.shape
firms = pd.read_csv("firmsus2023.csv")
firms.shape
firms.columns
# I keep only the columns I need: company code, company name, status and industry:
firms1 = firms[["empresa","Nombre","status","naics1"]]
firms1.columns=['firm','Empresa','status','industria']
# I do a left join using the panel data as the left dataset:
data = pd.merge(data, firms1, on="firm", how='left')
data['qdate'] = pd.PeriodIndex(data.q, freq="Q")
data.set_index(['firm','qdate'], inplace=True)
data.head()
data.index
# Annual log returns:
data['ry'] = np.log(data['adjprice']) - np.log(data.groupby(['firm'])['adjprice'].shift(4))
# Quarterly log returns:
data['rq'] = np.log(data['adjprice']) - np.log(data.groupby(['firm'])['adjprice'].shift(1))
# Note that I use groupby function to ensure that the returns of the first quarters of a firm are not calculated since the previous row belongs to other firm!
# Future quarterly and annual returns (1 quarter in the future):
data['f1rq'] = data.groupby(['firm'])['rq'].shift(-1)
data['f4rq'] = data.groupby(['firm'])['rq'].shift(-4)
data['f1ry'] = data.groupby(['firm'])['ry'].shift(-1)
data['f4ry'] = data.groupby(['firm'])['ry'].shift(-4)
# I will use the future returns as dependent variable, so I will be able to understand which current variables might be statistically related to future returns
# I check whether the return calculations are correct. I select rows where I can see 2 different firms and check whether returns and future returns where calculated correctly:
data[["fiscalmonth","adjprice","rq","ry","f1ry","f4ry"]].iloc[182:210]
quit
load("C:/ATec/202313/AIStats/.RData")
reticulate::repl_python()
reticulate::repl_python()
import pandas as pd
import yfinance as yf
import numpy as np
import statistics as st
import pandas as pd
import yfinance as yf
import numpy as np
import statistics as st
MXX = yf.download(tickers = "^MXX", start = "2000-01-02", interval="1d")
# I select the adjusted column:
MXX = MXX['Adj Close']
MXX
lnmxx = np.log(MXX)
N = len(lnmxx)
N
phi0 = (lnmxx[-1] - lnmxx[0]) / N
sigma = np.std(lnmxx) / np.sqrt(N)
sigma
y0 = lnmxx[0]
phi1 = 1
ysimln = []
ysimln.append(y0)
for i in range(1,N):
error=np.random.normal(0,sigma)
ysimln.append(phi0 + phi1 * ysimln[i-1] + error)
ysim = pd.DataFrame(lnmxx)
ysim['ysimln'] = ysimln
ysim
import matplotlib
from matplotlib.pyplot import *
clf()
plot(ysim['Adj Close'],color = 'r')
plot(ysim['ysimln'], color = 'b')
legend(['original log IPyC','Random wok with drift'],loc = 'upper left')
show()
ysim['IPyC'] =np.exp(ysim['Adj Close'])
ysim['simIPyC'] = np.exp(ysim['ysimln'])
clf()
plot(ysim['IPyC'],color = 'r')
plot(ysim['simIPyC'], color = 'b')
legend(['original IPyC','Random wok with drift'],loc = 'upper left')
show()
import pandas as pd
iaiqro = pd.read_csv("iaiqro.csv")
iaiqro = iaiqro.sort_values(by='Periodos').reset_index(drop=True)
iaiqro.shape
iaiqro
clf()
plot(iaiqro['iaiqro'])
title('Querétaro Industrial Activity')
show()
iaiqro['logiaiqro'] = np.log(iaiqro['iaiqro'])
clf()
plot(iaiqro['logiaiqro'])
title('Log of Querétaro Industrial Activity')
show()
iaiqro['anngrowth'] = iaiqro['logiaiqro'] - iaiqro['logiaiqro'].shift(12)
clf()
plot(iaiqro['anngrowth'])
title('Annual growth of Qro industrial activity')
show()
from statsmodels.tsa.stattools import adfuller
test1 = adfuller(iaiqro['anngrowth'].dropna())
# The pvalue is the second element of the tuple:
print(test1[1])
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
clf()
plot_acf(iaiqro['anngrowth'].dropna(), lags=12, zero=False)
show()
clf()
plot_pacf(iaiqro['anngrowth'].dropna(), lags=12 ,zero=False)
show()
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
model1 = SARIMAX(iaiqro['logiaiqro'].dropna(), order = (1, 0, 0), seasonal_order = (0, 1, 0, 12), trend = 'c')
model1fit = model1.fit()
print(model1fit.summary())
# I do not use the first 12 rows since there is no way to calculate predictions nor errors for the first 12 months since
#   I am using annual growth
errors = model1fit.resid.iloc[12:]
clf()
plot_acf(errors, lags=12, zero = False)
show()
model2 = SARIMAX(iaiqro['logiaiqro'].dropna(), order = (1, 0, 0), seasonal_order = (0, 1, 1, 12), trend = 'c')
model2fit = model2.fit()
print(model2fit.summary())
errors = model2fit.resid.iloc[12:]
clf()
plot_acf(errors, lags=12, zero = False)
show()
predictions = model2fit.predict(start=12, end=len(iaiqro))
# I forecast 6.5 years, from Jul 2023 to Dec 2030, which are 78 months
forecast = model2fit.forecast(78)
# I get the exponential to get forecast values in index (not in log)
predictions_iaiqro = np.exp(predictions)
forecast_iaiqro = np.exp(forecast)
clf()
plot(iaiqro['iaiqro'], color= 'g')
plot(predictions_iaiqro, color='r')
plot(forecast_iaiqro, color = 'b')
legend(['Real index','Predictions of index','Forecast of index'], loc = 'lower right')
show()
View(iaiqro)
type(iaiqro.Periodos)
type(iaiqro['Periodos'])
dtype(iaiqro['Periodos'])
iaiqro['Periodos'].dtype
iaiqro['Periodos'].dtype()
iaiqro.dtypes
iaiqro.dtypes
iaiqro.dtypes
iaiqro['Periodos'] = pd.to_datetime(iaiqro['Periodos'])
iaiqro['Periodos'].str.contains("/p1")
iaiqro['Periodos'].str.contains("/p1").sum()
iaiqro['Periodos'] = iaiqro['Periodos'].str.split().str[0]
iaiqro['Periodos'].str.contains("/p1").sum()
iaiqro['Periodos'] = pd.to_datetime(iaiqro['Periodos'])
iaiqro['Periodos'] = pd.to_datetime(iaiqro['Periodos'])
iaiqro.dtypes()
iaiqro.dtypes()
iaiqro.dtypes
iaiqro['Periodos'] = pd.to_datetime(iaiqro['Periodos'])
iaiqro['Periodos'] = pd.to_datetime(iaiqro['Periodos'])
iaiqro.dtypes
iaiqro['logiaiqro'] = np.log(iaiqro['iaiqro'])
clf()
plot(iaiqro['logiaiqro'])
title('Log of Querétaro Industrial Activity')
show()
iaiqro['anngrowth'] = iaiqro['logiaiqro'] - iaiqro['logiaiqro'].shift(12)
clf()
plot(iaiqro['anngrowth'])
title('Annual growth of Qro industrial activity')
show()
iaiqro['Periodos'] = iaiqro.index
iaiqro
import pandas as pd
import yfinance as yf
import numpy as np
import statistics as st
MXX = yf.download(tickers = "^MXX", start = "2000-01-02", interval="1d")
# I select the adjusted column:
MXX = MXX['Adj Close']
MXX
lnmxx = np.log(MXX)
N = len(lnmxx)
N
phi0 = (lnmxx[-1] - lnmxx[0]) / N
sigma = np.std(lnmxx) / np.sqrt(N)
sigma
y0 = lnmxx[0]
phi1 = 1
ysimln = []
ysimln.append(y0)
for i in range(1,N):
error=np.random.normal(0,sigma)
ysimln.append(phi0 + phi1 * ysimln[i-1] + error)
ysim = pd.DataFrame(lnmxx)
ysim['ysimln'] = ysimln
ysim
import matplotlib
from matplotlib.pyplot import *
clf()
plot(ysim['Adj Close'],color = 'r')
plot(ysim['ysimln'], color = 'b')
legend(['original log IPyC','Random wok with drift'],loc = 'upper left')
show()
ysim['IPyC'] =np.exp(ysim['Adj Close'])
ysim['simIPyC'] = np.exp(ysim['ysimln'])
clf()
plot(ysim['IPyC'],color = 'r')
plot(ysim['simIPyC'], color = 'b')
legend(['original IPyC','Random wok with drift'],loc = 'upper left')
show()
import pandas as pd
iaiqro = pd.read_csv("iaiqro.csv")
iaiqro = iaiqro.sort_values(by='Periodos').reset_index(drop=True)
iaiqro.shape
iaiqro
clf()
plot(iaiqro['iaiqro'])
title('Querétaro Industrial Activity')
show()
iaiqro.dtypes
iaiqro['Periodos'] = pd.to_datetime(iaiqro['Periodos'])
iaiqro['Periodos'].str.contains("/p1").sum()
iaiqro['Periodos'] = iaiqro['Periodos'].str.split().str[0]
iaiqro['Periodos'].str.contains("/p1").sum()
iaiqro['Periodos'] = pd.to_datetime(iaiqro['Periodos'])
iaiqro.dtypes
iaiqro.set_index('Periodos')
iaiqro
iaiqro.set_index('Periodos', inplace=True)
#iaiqro['Periodos'] = iaiqro.index
iaiqro
iaiqro['logiaiqro'] = np.log(iaiqro['iaiqro'])
clf()
plot(iaiqro['logiaiqro'])
title('Log of Querétaro Industrial Activity')
show()
iaiqro['anngrowth'] = iaiqro['logiaiqro'] - iaiqro['logiaiqro'].shift(12)
clf()
plot(iaiqro['anngrowth'])
title('Annual growth of Qro industrial activity')
show()
from statsmodels.tsa.stattools import adfuller
test1 = adfuller(iaiqro['anngrowth'].dropna())
# The pvalue is the second element of the tuple:
print(test1[1])
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
clf()
plot_acf(iaiqro['anngrowth'].dropna(), lags=12, zero=False)
show()
clf()
plot_pacf(iaiqro['anngrowth'].dropna(), lags=12 ,zero=False)
show()
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
model1 = SARIMAX(iaiqro['logiaiqro'].dropna(), order = (1, 0, 0), seasonal_order = (0, 1, 0, 12), trend = 'c')
model1fit = model1.fit()
print(model1fit.summary())
