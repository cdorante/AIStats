---
title: "Workshop 4, Advanced AI - Statistics Module"
bibliography: references.bib
author:  
 - Alberto Dorantes D., Ph.D.
 - Monterrey Tech, Queretaro Campus

abstract: In this workshop we continue learning about the Simple Linear Regression Model.

editor: visual
jupyter: python3
format:
  html: 
    toc: true
    toc-depth: 4
    toc-title: Content
    toc-location: left
    toc_float:
      collase: false
    code-fold: false
    theme: united
    highlight-style: breezedark
    number-sections: true
    fontsize: 1.1em
    linestretch: 1.7
---

# Workshop Directions

You have to work on Google Colab for all your workshops. In Google Colab, you **MUST LOGIN** with your \@tec.mx account and then create a google colab document for each workshop.

You **must share** each Colab document (workshop) with the following accounts:

-   cdorante.tec\@gmail.com
-   cdorante\@tec.mx

You must give **Edit** privileges to these accounts.

You have to follow this workshop in class to learn about topics. You have to do your own Markdown note/document for every workshop we cover.

Rename your Notebook as "W4-Statistics-AI-YourFirstName-YourLastname".

You **must submit** your workshop before we start with the next workshop. What you have to write in your workshop? You have to:

-   You have to **REPLICATE** and **RUN** all the Python code, and

-   DO ALL **CHALLENGES** stated in sections. These challenges can be Python code or just responding **QUESTIONS** with your own words and in **CAPITAL LETTERS**. You have to **WRITE CLEARLY** so that I can see your **LINE OF THINKING!**

The submissions of your workshops is a **REQUISITE** for grading your final deliverable document of the Statistics Module.

I strongly recommended you to write your OWN NOTES about the topics as if it were your study NOTEBOOK.

# Introduction

Up to know we have learn about

-   Descriptive Statistics

-   The Histogram

-   The Central Limit Theorem

-   Hypothesis Testing

-   Covariance and Correlation

Without the idea of summarizing data with descriptive statistics, we cannot conceive the histogram. Without the idea of the histogram we cannot conceive the CLT, and without the CLT we cannot make inferences for hypothesis testing. We can apply hypothesis testing to test claims about random variables. These random variables can be one mean, difference of 2 means, correlation, and also coefficients of the **linear regression model**. But what is the linear regression model?

We learned that covariance and correlation are measures of linear relationship between 2 random variables, X and Y. The simple regression model also measures the linear relationship between 2 random variables (X and Y), but the difference is that X is supposed to explain the movements Y, so Y depends on the movement of X, the independent variable. In addition, the regression model estimates a linear equation (regression line) to represent how much Y (on average) moves with movements of X, and what is the expected value of Y when X=0.

The simple linear regression model is used to understand the ***linear relationship*** between two variables assuming that one variable, the independent variable (IV), can be used as a predictor of the other variable, the dependent variable (DV).

Besides using linear regression models to better understand how the dependent variable moves or changes according to changes in the independent variable, linear regression models are also used for prediction or forecasting of the dependent variable.

The simple regression model considers only one independent variable, while the multiple regression model can include more than one independent variables. But both models only consider one dependent variable. Then, we can use regression models for:

• Understanding the relationship between a dependent variable and a one or more independent variables - also called explanatory variables

• Predicting or estimating the expected value of the dependent variable according to specific value of the independent variables

In regression models it is assumed that there is a linear relationship between the dependent variable and the independent variables. It might be possible that in reality, the relation is not linear. A linear regression model does not capture non-linear relationships unless we do specific mathematical transformations of the variables.

You must read my Lecture Note: [**Basics of Linear Regression Models in the context of Finance**](https://docs.google.com/viewer?a=v&pid=sites&srcid=ZWdhZGVyemMubmV0fGVjMjAwM3xneDozMjA2YmVjYjZjMmVjODA5). In this note I explain in detail the Linear Regression model, how it is estimated and how we can make inferences with the model.

In this workshop I illustrate the simple regression model with an example from Finance: the Market Regression Model.

The Market Model states that the expected return of a stock is given by its alpha coefficient ($α$) plus its market beta coefficient ($β$) times the market return. In mathematical terms:

$$ 
E[R_i] = α + β(R_M) 
$$

We can express the same equation using $β_0$ as alpha, and $β_1$ as market beta:

$$
E[R_i] = β_0 + β_1(R_M)
$$

We can estimate the $β_0$ and $β_1$ coefficients by running a simple linear regression model specifying that the market return is the independent variable and the stock return is the dependent variable. It is strongly recommended to use continuously compounded returns instead of simple returns to estimate the market regression model.

The market regression model can be expressed as:

$$
r_{(i,t)} = b_0 + b_1*r_{(M,t)} + ε_t 
$$

Where:

$ε_t$ is the error at time t. Thanks to the Central Limit Theorem, this error behaves like a Normal distributed random variable ∼ N(0, $σ_ε$) since the error is actually a linear combination of random variables; the error term $ε_t$ is expected to have mean=0 and a specific standard deviation $σ_ε$.

$r_{(i,t)}$ is the return of the stock i at time t.

$r_{(M,t)}$ is the market return at time t

$b_0$ and $b_1$ are called regression coefficients

# Interesting facts from history

One of the most common methods to estimate linear regression models is called ordinary least squares, which was first developed by mathematicians to predict planets' orbits. On January 1st, 1801, the Italian priest and astronomer Giuseppe Piazzi discovered a small planetoid (asteroid) in our solar system, which he named Ceres. Piazzi observed and recorded 22 Ceres positions during 42 days, but suddenly Ceres was lost in glare of the Sun. Then, most Europeans astronomers started to find out a way to predict Cere's orbit. The great German mathematician Friedrich Carl Gauss successfully predicted Ceres' orbit using a least squares method he had developed in 1796, when he was 18 years old. Gauss applied his least squares method using the 22 Ceres observations and 6 explanatory variables. Gauss published his least square method until 1809 [@Gauss1809]; interestingly, the French mathematician Arien-Marie Legendre first published the least-squared method in 1805 [@Legendre1805].

About 70 years later, the English anthropologist Francis Galton and the English mathematician Karl Pearson - leading founders of the Statistics discipline- used the foundations of the least-square method to first develop the linear regression model. Galton developed the conceptual foundation of regression models when he was studying the inherited characteristics of sweet peas. Pearson further developed Galton ideas following rigurous mathematical development.

Pearson used to work in Galton's laboratory. When Galton died, Pearson wrote Galton's biography. In this biography [@Pearson1930], Pearson described how Galton came up with the idea of regression. In 1875 Galton gave sweet peas seeds to seven friends. All sweet peas seeds had uniform weights. His friends harvested the sweet peas and returned the plants to Galton. He did a graph to see the size of each plant compared with their respective parents' sizes. He found that all of them had parents with higher size. When graphing the offspring's size as the Y axis, and parents' size as the X axis, he tried to manually draw a line that could represent this relationship, and he found that the line slope was less than 1.0. He concluded that the size of these plants in their generation was "regressing" to the supposed mean of this specie (considering several generations).

Two research articles by Galton [@Galton1886] and Pearson [@Pearson1930] written in 1886 and 1903 respectively further developed the foundations of regression models. They examined why sons of very tall fathers are usually shorter than their fathers, while sons of very short fathers are usually taller than their fathers. After collecting and analyzing data from hundreds of families, they concluded that the height of an individual in a community or population tends to "regress" to the average height of the such population where they were born. If the father is very tall, then his sons' height will "regress" to the average height of such population. If the father is very short, then his sons' height will also "regress" to such average height. They named their model as "regression" model. Nowadays the interpretation of regression models is not quite the same as "regress" to a specific average value. Nowadays regression models are used to examine linear relationships between a dependent variable and a set of independent variables.

# Types of data structures

The market model is a time-series regression model. In this model we looked at the relationship between 2 variables representing one feature or attribute (returns) of two "subjects" over time: a stock and a market index. The market model is an example of a regression model, but the data structure or type of data used is time-series data. These type of regression models are called pulled time-series regression.

There are basically three types of data used in regression models:

-   Time-series: each observation represents one period, and each column represents one or more variables, which are characteristics of one or more subjects. Then, we have one or more variables measured in several time periods.

-   Cross-sectional: each observation represents one subject in only **one time period**, and each column represents variables or characteristics of the subjects.

-   Panel data: this is a combination of time-series with cross-sectional structure.

Then, we can consider the market model as a pulled "time-series" regression model.

Another way to classify regression models is based on the number of independent variables. If the regression model considers only one independent variable, the the model is known as simple regression model. If the model considers more than one independent variable, the model is known as multiple regression model.

# The OLS method

The Ordinary Least Square is an optimization method to estimate the best values of the beta coefficients and their standard errors in a linear regression model.

In the case of simple regression model, we need to estimate the best values of beta0 (the intercept) and beta1 coefficient (the slope of the regression line). If Y is the dependent variable and X the independent variable, the regression equation is as follows:

$$
y_i = b_0 + b_1(x_i) + \epsilon_i
$$ The **regression line** is given by the expected value of Y (also called $\hat{Y}$):

$$
E[y_i] = b_0 + b_1(x_i) = \hat{y}_i
$$

In this case, i goes from 1 to N, the total number of observations of the sample.

If we plot all pairs of $(x_i,y_i)$ we can first visualize whether there is a linear relationship between Y and X. In a scatter plot, each pair of $(x_i,y_i)$ is one point in the plot.

The purpose of OLS is to find the best regression line that best represents all the points$(x_i,y_i)$. The beta0 and beta1 coefficients define the regression line. If beta0 changes, then the intercept of the line moves. If beta1 changes, then the slope of the line changes.

To find the best regression line (beta0 and beta1), the OLS method tries to minimize the sum of squared errors of the regression equation. Then OLS is an optimization method with the following objective:

Minimize $\sum_{i=1}^{N}(y_{i}-\hat{y}_{i})^{2}$

If we replace $\hat{Y}_i$ by the regression equation we get:

Minimize $\sum_{i=1}^{N}\left[y_{i}-(b_{0}+b_{1}*x_{i})\right]^{2}$

Then, this sum can be seen as a function of $b_0$ and $b_1$. If $b_0$ changes, this sum changes; the same with $b_1$. Then we can re-write the optimization objective as:

Minimize:

$$
f(b_{0},b_{1})=\sum_{i=1}^{N}\left[y_{i}-(b_{0}+b_{1}*x_{i})\right]^{2}
$$

This is a quadratic function of 2 variables. If we get its 2 first partial derivatives (with respect to $b_0$ and $b_1$) and make them equal to zero we will get 2 equations and 2 unknowns, and the solution will be the optimal values of $b_0$ and $b_1$ that minimizes this sum of squared errors. The set of these 2 partial derivatives is also called the **gradient** of the function.

If you remember basic geometry, if we imagine possible values of $b_0$ in the X axis and values of $b_1$ in the Y axis, then this function is actually a single-curved area (surface). The optimal point ($b_0, b_1$) will be the lowest point of this curved surface.

Let's do an example with a dataset of only 4 observations:

|  X  |  Y  |
|:---:|:---:|
|  1  |  2  |
|  2  | -1  |
|  7  | 10  |
|  8  |  7  |

If I do a scatter plot and a line that fits the points:

```{python}
#| code-fold: true
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
data = {'x':  [1,2,7,8],
        'y': [2,-1,10,7]}
df = pd.DataFrame(data)
b1,b0 = np.polyfit(df.x,df.y,1)
df['yhat'] = b0 + b1*df['x']
#plt.clf()
plt.scatter(df.x,df.y)
plt.plot(df.x, df.yhat,c="orange")
plt.xticks(np.arange(-4,14,1))
plt.yticks(np.arange(-2,11,1))

for i in range(4):
  x=df.x.iloc[i]
  ymin= df.y.iloc[i]
  ymax=df.yhat.iloc[i]
  if (ymin>ymax):
    temp=ymax
    ymax=ymin
    ymin=temp
  plt.vlines(x=x,ymin=ymin,ymax=ymax,color='r')

plt.axhline(y=0)
plt.axvline(x=0)

plt.xlabel("X")
plt.ylabel("Y")
plt.grid()
plt.show()

```

The *error* of each point is the **red vertical line**, which is the distance between the point and the prediction of the regression line. This distance is given by the difference between the specific $y_i$ value and its predicted value:

$$
\epsilon_i=(y_i - \hat{y}_i)
$$

In this example, 2 errors are positive and 2 are negative.

The purpose of OLS is to find the values of $b_0$ and $b_1$ such that the sum of the squared of all errors is minimized. Mathematically there is only one solution for a specific set of X and Y values.

Let's return to the objective function, which is determined by both coefficients, $b_0$ and $b_1$.

We can do a 3D plot for this function to have a better idea. It is a 3D plot since the function depends on 2 values: $b_0$ and $b_1$:

```{python}
#| code-fold: true

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import collections
# I define a function to get the sum of squared errors given a specific b0 and b1 coefficients:
def sumsqerrors2(b1, b0,df):
    return sum( ( df.y - (b0+b1*df.x)) **2)
# Note that df is a dataframe, so this line of code performs a row-wise operation to avoid 
#   writing a loop to sum each squared error for each observation

# Create the plot:
fig = plt.figure()

ax = fig.add_subplot(1,1,1, projection='3d')
# I create 20 possible values of beta0 and beta1:
# beta1 will move between -1 and 3
b1s = np.linspace(-1, 3.0, 20)
# beta0 will move between -2 and 2:
b0s = np.linspace(-2, 2, 20)
# I create a grid with all possible combinations of beta0 and beta1 using the meshgrid function:
# M will be all the b1s values, and B the beta0 values:
M, B = np.meshgrid(b1s, b0s)
# I calculate the sum of squared errors with all possible pairs of beta0 and beta1 of the previous grid:
zs = np.array([sumsqerrors2(mp, bp, df) 
        for mp, bp in zip(np.ravel(M), np.ravel(B))])
# I reshape the zs (squared errors) from a vector to a grid of the same size as M (20x20)
Z = zs.reshape(M.shape)

ax.plot_surface(M, B, Z, rstride=1, cstride=1, color='b', alpha=0.5)

ax.set_xlabel('b1')
ax.set_ylabel('b0')
ax.set_zlabel('sum sq.errors')

plt.show()
```

We see that this function is single-curved. The sum of squared errors changes with different paired values ($b_0$,$b_1$). The lowest point of this surface will be the optimal values of ($b_0$, $b_1$) where the sum of squared error is the minimum of all. Then, how can we calculate this optimal point?

Remembering a little bit of Calculus and simultaneous equations, we can do the following:

-   Take the partial derivatives of the function with respect to its variables $b_0$ and $b_1$ and make it equal to zero:

$$
f(b_{0},b_{1})=\sum_{i=1}^{N}\left[y_{i}-(b_{0}+b_{1}*x_{i})\right]^{2}
$$

$$
\frac{\delta}{b_{0}}f(b_{0},b_{1})=2*(-1)*\sum_{i=1}^{N}\left[y_{i}-(b_{0}+b_{1}*x_{i})\right]=0
$$

Dividing both sides by -2 we get:

$$
\sum_{i=1}^{N}\left[y_{i}-(b_{0}+b_{1}*x_{i})\right]=0
$$ {#eq-firsteq}

Since $\left[(y_{i}-(b_{0}+b_{1}*x_{i})\right]$ is the error for the i observation, then this @eq-firsteq states that **the sum of all errors must be equal to zero**.

Let's do the same for the other partial derivative:

$$
\frac{\delta}{b_{1}}f(b_{0},b_{1})=2*\sum_{i=1}^{N}\left[y_{i}-(b_{0}+b_{1}*x_{i})\right]*(-x_i)=0
$$

Dividing both sides by -2 both:

$$
\sum_{i=1}^{N}(x_i)\left[y_{i}-(b_{0}+b_{1}*x_{i})\right]=0
$$ {#eq-seceq}

Now we have 2 equations with 2 unknowns. I this case, the unknown variables are $b_0$ and $b_1$, since the X and Y values are considered as given since the start of the problem.

-   Solve the systems of equations

We can further simplify both equations by applying the sum operator to each term:

For @eq-firsteq:

$$
\sum_{i=1}^{N}y_{i}-N(b_{0})-b_{1}*\sum_{i=1}^{N}x_{i}=0
$$

Since $\bar{y}=\frac{1}{N}\sum_{i=1}^Ny_{i}$, then $\sum_{i=1}^{N}y_{i}=N*\bar{y}$, and the same for the x variable, then:

$$
N(\bar{y})-N(b_0)-b1(N)(\bar{x})=0
$$ Dividing both sides by N:

$$
\bar{y}-b_0-b_1(\bar{x})=0
$$

This is another way to express @eq-firsteq. This form of the equation indicates that **the point** $(\bar{x},\bar{y})$ must be in the regression line since the error at this point is equal to zero!

From previous equation, then we get that:

$$
b_0=\bar{y} - b_1\bar{x}
$$

After applying the sum to each term of @eq-seceq, it can be express as:

$$
\sum_{i=1}^{N}x_{i}y_{i}-b_{0}\sum_{i=1}^{N}x_{i}-b_{1}\sum_{i=1}^{N}x_{i}^2=0
$$

$$
\sum_{i=1}^{N}x_{i}y_{i}-(b_{0})N\bar{x}-b_{1}\sum_{i=1}^{N}x_{i}^2=0
$$

Now we plug the $b_0$ formula from @eq-firsteq in this form of @eq-seceq:

$$
\sum_{i=1}^{N}x_{i}y_{i}-(\bar{y} - b_1\bar{x})N\bar{x}-b_{1}\sum_{i=1}^{N}x_{i}^2=0
$$ Factorizing $b_1$:

$$
\sum_{i=1}^{N}x_{i}y_{i} - b_1(\sum_{i=1}^{N}x_{i}^2-N\bar{x}^2)-N\bar{x}\bar{y}=0
$$

Finally, we get the optimal value of $b_1$ as:

$$
b_1=\frac{\sum_{i=1}^{N}x_{i}y_{i}-N\bar{x}\bar{y}}{\sum_{i=1}^{N}x_{i}^2-N\bar{x}^2}
$$

If we divide the numerator and denominator by $1/N$ we get a formula for $b_1$ that is given in terms of variance and covariance:

$$
b_1=\frac{\frac{1}{N}\sum_{i=1}^{N}x_{i}y_{i}-\bar{x}\bar{y}}{\frac{1}{N}\sum_{i=1}^{N}x_{i}^2-\bar{x}^2}=\frac{E[xy]-\bar{x}\bar{y}}{E[x^2]-\bar{x}^2}
$$

$$
b_1=\frac{Cov(x,y)}{Var(x)}
$$ 

Interestingly, from this final formula for $b_1$, which is the slope of the regression line, we can see that $b_1$ is **how much y covariates with x with respect to the variability of x**. This is actually the concept of a slope of a line! it is the **sensitivity** of how much y changes when x changes. Then, $b_1$ can be seen as the expected rate of change of y with respect to x, which is a derivative!

How the $b_1$ coefficient and the correlation between x and y are related? We learned that correlation measures the linear relationship between 2 variables. Also, $b_1$ measures the linear relationship between 2 variables, however, there is an important difference. Let's see the correlation formula:

$$
Corr(x,y)=\frac{Cov(x,y)}{SD(x)SD(y)}
$$

We can express Covariance in terms of Correlation:

$$
Cov(x,y)=Corr(x,y)SD(x)SD(y)
$$

If we plug this formula in the $b_1$ formula:

$$
b_1=Corr(x,y)*\frac{SD(y)}{SD(x)}
$$

Then, we can see that $b_1$ is a type of *scaled correlation* since it is that is scaled by the ratio of both standard deviations, so $b_1$ provides information not only about relationship, but also about sensitivity of how much y changes in magnitude for each change in 1 unit of x!

# CHALLENGE: Estimate a market regression model

Now it's time to use real data to better understand this model. Download monthly prices for Alfa (ALFAA.MX) and the Mexican market index IPCyC (\^MXX) from Yahoo Finance from January 2018 to July 2022.

You have to do the following:

-   Calculate cc returns of both stocks and drop NA values

```{python}
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt

# Getting price data and selecting adjusted price columns:
sprices=yf.download(tickers="ALFAA.MX,^MXX", start="2019-01-01", end = "2024-07-31",interval="1mo")

sprices = sprices['Adj Close']

# Calculating cc returns:
returns = np.log(sprices) - np.log(sprices.shift(1))
# Deleting the first month with NAs:
returns=returns.dropna()
# I rename the columns; the first column is Alfa returns:
returns.columns=['ALFAAret','MXXret']
# I view the first and last returns:
print(returns.head())
print(returns.tail())
```

We do a scatter plot including the regression line:

```{python}
import seaborn as sb
plt.clf()
x = returns['MXXret']
y = returns['ALFAAret']
# I plot the (x,y) values along with the regression line that best fits the data:
sb.regplot(x=x,y=y)
plt.xlabel('Market returns')
plt.ylabel('Alfa returns')
plt.show()
```

Scatter plots can be misleading when ranges of X and Y are very different. In this case, Alfa had a wider range of return values compared to the Market returns. Alfa has offered very negative returns from -60% to about +40%, while the Market returns had offered returns from about -17% to + 12%.

Then, we can re-do the scatter plot trying to make the X and Y axis using a similar scale fro both variables:

```{python}
plt.clf()
sb.regplot(x=x,y=y)
# I adjust the scale of the X axis so that the magnitude of each unit of x is equal to that of the Y axis:
plt.xticks(np.arange(-1,1,0.2))
# I label the axis:
plt.xlabel("Market returns")
plt.ylabel("Alfa returns")
plt.show()
```

## CHALLENGE

**WHAT DOES THE PLOT TELL YOU? BRIEFLY EXPLAIN**

## ESTIMATING THE MARKET REGRESSION MODEL

The OLS function from the satsmodel package is used to estimate a regression model. We run a simple regression model to see how the monthly returns of the stock are related with the market return.

The first parameter of the OLS function is the DEPENDENT VARIABLE (in this case, the stock return), and the second parameter must be the INDEPENDENT VARIABLE, also named the EXPLANATORY VARIABLE (in this case, the market return).

Before we run the OLS function, we need to add a column of 1’s to the X vector in order to estimate the beta0 coefficient (the constant).

What you will get is called the Market Regression Model. You are trying to examine how the market returns can explain stock returns:

```{python}

import statsmodels.api as sm
# I add a column of 1's to the X dataframe in order to include the beta0 coefficient (intercept) in the model:
X = sm.add_constant(x)
# I estimate the OLS regression model:
mkmodel = sm.OLS(y,X).fit()
# I display the summary of the regression: 
print(mkmodel.summary())
```

```{python}
#| code-fold: true
# I can also run the OLS regression using the ols function 
import statsmodels.formula.api as smf

mkmodel2 = smf.ols('ALFAAret ~ MXXret',data=returns).fit()
# This function does not require to add the column of 1's to include the intercept!
print(mkmodel2.summary())
```
We now estimate theregression coefficients using Matrix Algebra

```{python}
# Using matrix algebra to estimate the beta coefficients:
# I add the column of 1's to the dataframe:
returns['constant'] = 1
selcols = ['constant','MXXret']
# I set x as a matrix with the column of 1's and the values of X:
x = returns[selcols].values
# I set y as the dependent variable:
y = returns['ALFAAret'].values
# I calculate the matrix multiplication X'X:
xtx = np.matmul(x.transpose(),x)
# I calculate the matrix multiplication X'Y:
xty = np.matmul(x.transpose(),y)
# I get the inverse of the matrix (X'X) to solve for the beta coefficients:
invtxt = np.linalg.inv(xtx)
# I multiply inv(X'X)*X'Y to get the estimation of the beta vector (beta0 and beta1 coefficients) 
betas = np.matmul(invtxt,xty)
betas
```

The regression equation according to the previous results is: 

```{python}
#| echo: false
b0 = mkmodel.params[0]
b1 = mkmodel.params[1]
from IPython.display import display, Markdown
display(Markdown("""
The regression equation is: E[ALFAret]= {b0} + {b1}*MXXret.
""".format(b0=b0,b1=b1)))
```

In the regression outputs we see that both beta coefficients have not only their optimal values, but also their standard error, t-Statistic, p-values, and their 95% confidence intervals. Note that the regression function in Python automatically performs hypothesis testing for both coefficients, $b_0$ and $b_1$, where the null hypotheses are that the coefficients are equal to zero

In the next section we will learn about these estimations.

# The standard error of the beta coefficients

The OLS method includes the estimation of the beta coefficients and also estimation of their corresponding standard errors. Then, what is the standard error of the beta coefficients?

The standard error of $b_0$ is actually an estimation of the **expected standard deviation of** $b_0$. The same happens for $b_1$. But how we can estimate a standard deviation of a beta coefficient? It seems that we need several possible values of $b_0$ and $b_1$, so I could estimate their standard deviation as a measure of standard error. Then, we would need many samples to estimate several possible pair values of beta coefficients. However, most of the time we only use 1 sample to estimate the beta coefficients and their standard errors. Then, why do we need to estimate the expected standard deviation of the coefficients?

In several disciplines, when we want to understand the relationship between 2 random variables, we only have access to 1 sample (not the population), so we need to find a way to estimate the error levels we might have with the beta estimations. Then, we need to find a way to estimate the **possible variation of the beta coefficients** as if we would had the possibility to collect many samples to get many possible pairs of beta coefficients.

It sound weird, but it is possible to estimate how much the $b_0$ and $b_1$ might vary using only 1 sample. We can use basic probability theory and the result of the **Central Limit Theorem** to estimate the **expected standard deviation of the beta coefficients**, their corresponding t-values, p-values and their 95% confidence intervals.

I will not derive here the formulas for the expected standard deviation of the beta coefficients, but you can check this derivation in the Appendix of my note: [**Basics of Linear Regression Models in the context of Finance**](https://drive.google.com/file/d/1BLOpS05ix2csJ-GNn9yukDTAnnNl-6TF/view?usp=sharing)

To estimate the standard errors of the coefficients, we need to estimate the standard deviation of the Y variable, which is actually the standard deviation of the errors, and it is also called mean squared errors (MSE).

The formula to estimate the standard deviation of the regression errors, mean squared errors (MSE), is:

$$
MSE=\sqrt{\frac{SSE}{N-2}}
$$ 

Where: SSE = Sum of squared errors:

The SSE is divided by N-2 since we need 2 parameters ($b_0$ and $b_1$) to estimate SSE.

The SSE is calculated as:

$$
SSE = \sum_{i=1}^{N}(y_i-\hat{y})^2
$$

The formula to estimate the standard error (SE) of $b_1$ is the following:

$$
SE(b_1)=\frac{MSE}{\sqrt{\sum_{i=1}^N(x_i-\bar{x})^2}}
$$

The formula to estimate the standard error (SE) of $b_0$ is the following:

$$ 
SE(b_0)=MSE\sqrt{\frac{\sum_{i=1}^{N}x_i^2}{N\sum_{i=1}^N(x_i-\bar{x})^2}}
$$

These estimations are automatically calculated and displayed in the output of any regression software. By learning their formulas we see that the magnitude of standard error of both coefficients is directly proportional to the **standard deviation of the the errors**, which is the mean of squared regression errors (MSE). Then, the greater the individual magnitude of errors, the greater the standard error of both beta coefficients, so the more difficult to find significant betas (betas that are significantly different than zero).

To further understand the standard error of beta coefficients we will do an exercise to estimate several regression models using different time ranges. Each regression will estimate one value for $b_0$ and one value for $b_1$. Then, if we run N regressions, we will have N pairs of beta coefficients, so we will see how these beta coefficients change over time. This change is measured by the standard error of the beta coefficients.

# Estimate moving betas for the market regression model

How the beta coefficients of a stock move over time? Are the $b_1$ and $b_0$ of a stock stable? if not, do they change gradually or can they radically change over time? We will run several rolling regression for Alfa to try to respond these questions.

Before we do the exercise, I will review the meaning of the beta coefficients in the context of the market model.

In the market regression model, $b_1$ is a measure of the sensitivity; it measures how much the stock return might move (on average) when the market return moves in +1%.

Then, according to the market regression model, the stock return will change if the market return changes, and also it will change by many other external factors. The aggregation of these external factors is what the error term represents.

It is said that $b_1$ in the market model measures **the systematic risk** of the stock, which depends on changes in the market return. **The unsystematic risk** of the stock is given by the error term, that is also named the **random shock**, which is the summary of the overall reaction of all investors to news that might affect the stock (news about the company, its industry, regulations, national news, global news).

We can make predictions of the stock return by measuring the systematic risk with the market regression model, but we cannot predict the unsystematic risk. The most we can measure with the market model is the variability of this unsystematic risk (the variance of the error).

In this exercise you have to estimate **rolling regressions** by moving time windows and run 1 regression for each time window.

For the same ALFAA.MX stock, run rolling regressions using a time window of 36 months, starting from Jan 2010.

The first regression has to start in Jan 2010 and end in Dec 2012 (36 months). For the second you have to move time window 1 month ahead, so it will start in Feb 2010 and ends in Jan 2013. For the third regression you move another month ahead and run the regression. You continue running all possible regressions until you end up with a window with the last 36 months of the dataset.

This sounds complicated, but fortunately we can use the function RollingOLS that automatically performs rolling regressions by shifting the 36-moth window by 1 month in each iteration.

Then, you have to do the following:

1.  Download monthly stock prices for ALFAA.MX and the market (\^MXX) from Jan 2010 to Jul 2022, and calculate cc returns.

```{python}

# Getting price data and selecting adjusted price columns:
sprices = yf.download("ALFAA.MX ^MXX",start="2010-01-01",interval="1mo")
sprices = sprices['Adj Close']

# Calculating returns:
returns = np.log(sprices) - np.log(sprices.shift(1))
# Deleting the first month with NAs:
returns=returns.dropna()
returns.columns=['ALFAAret','MXXret']
```

2.  Run rolling regressions and save the moving $b_0$ and $b_1$ coefficients for all time windows.

```{python}
from statsmodels.regression.rolling import RollingOLS
x=sm.add_constant(returns['MXXret'])
y = returns['ALFAAret']
rolreg = RollingOLS(y,x,window=36).fit()
betas = rolreg.params
# I check the last pairs of beta values:
betas.tail()

```

3.  Do a plot to see how $b_1$ and $b_0$ has changed over time.

```{python}
plt.clf()
plt.plot(betas['MXXret'])
plt.title('Moving beta1 for Alfaa')
plt.xlabel('Date')
plt.ylabel('beta1')
plt.show()
```

```{python}
plt.clf()
plt.plot(betas['const'])
plt.title('Moving beta0 for Alfaa')
plt.xlabel('Date')
plt.ylabel('beta0')
plt.show()
```

We can see that the both beta coefficients move over time; they are not constant. There is no apparent pattern for the changes of the beta coefficients, but we can appreciate how much they can move over time; in other words, we can visualize their standard deviation, which is the average movement from their means.

We can actually calculate the mean and standard deviation of all these pairs of moving beta coefficients and see how they compare with their beta coefficients and their standard errors of the original regression when we use only 1 sample with the last 36 months:

```{python}
betas.describe()
```

We calculated 116 regressions using 116 36-month rolling windows. For each regression we calculated a pair of $b_0$ and $b_1$.

Compared with the first market regression of Alfa using the most recent months from 2018 (about 54 months or 4.5 years), we see that the mean of the moving betas is very similar to the estimated beta of the first regression. Also, we see that the standard deviation of the moving $b_0$ is very similar to the standard error of $b_0$ estimated in the first regression. The standard deviation of $b_1$ was much higher than the standard error of $b_1$ of the first regression. This difference might be because the moving betas were estimated using data from 2010, while the first regression used data from 2018, so it seems that the systematic risk of Alfa (measured by its $b_1$) has been reducing in the recent months.

I hope that now you can understand why we need an estimation of the standard error of the beta coefficients (standard deviation of the coefficients).

Next we will learn how to use the estimated beta coefficient and their corresponding standard errors to calculate their corresponding t-Statistic, p-value and their 95% confidence interval.

# t-Statistic, p-value and 95% confidence interval of beta coefficients

## Hypothesis tests for the beta coefficients

When we run a linear regression model, besides the estimation of the beta coefficients and their corresponding standard errors, one **hypothesis test** is performed for each beta coefficient.

We apply the **hypothesis test to each of the beta coefficients** to test whether the beta coefficient is or is not **equal to zero**.

For the case of the simple market regression, the following hypothesis are performed:

For $b_0$:

H0: The mean of $b_0$ = 0\
Ha: The mean of $b_0$ \<\> 0 (Our hypothesis)

In this case, **the variable of study for the hypothesis test is the** $b_0$ coefficient.

Then, we calculate the t-Statistic for $b_0$ as follows:

$$
t =\frac{(b_0 - 0)}{SE(b_0)}
$$

$SE(b_0$)\$ is the standard error of $b_0$, which is its estimated standard deviation.

Remember that the t-Statistic is the standardized distance from $b_0$ (the value estimated from the regression) and zero. In other words, the t-Statistic tells us how many standard deviations of the $b_0$ the actual value of $b_0$ is away from zero, which is the hypothetical true value.

Remember that the null hypothesis (H0) is the hypothesis of the skeptical person who believes that $b_0$ is equal to zero. Then, we start assuming that H0 is true. If we show that there is very little probability (its p-value) that the $b_0$=0, then we will have statistical evidence to reject the H0 and support our Ha.\
Then, if $\mid t\mid$\>2, then we will have statistical evidence at least at the 95% confidence level to reject the null hypothesis. The critical value of 2 for t is an approximation; it depends on the \# of observations of the regression that this critical value can move from around 1.8 and 2.1.

From a t-Statistic and the \# of observations, we can estimate the exact p-value. This value cannot be calculated using a formula since there is no close solution for the t cumulative density function. However, remember that the t-Student probability distribution becomes very similar to the normal probability distribution when the \# of observations is equal or greater than 30. Then, if the t-Statistic is about 2, then if we remember the characteristic of the probability density function of a normal distribution, then the area under the curve (which is the probability) beyond t=2 and less than t=-2 will be around 0.05 (5%). This is the 2-sided p-value of the test.

Remember that the **p-value is the probability of making a mistake if we reject the null hypothesis.** Then, the less the p-value, the better. The rule of thumb is that if the p-value\<0.05, then we have statistical evidence at least at the 95% confidence level to reject the null.

Fortunately, the standard error, t-Statistic and the 2-sided p-value for this hypothesis test is automatically calculated and shown when we run a regression model.

Then, in conclusion, **if the p-value estimated for** $b_0$\<0.05 and $b_0$\>0, then we can say that there is statistical evidence at the 95% confidence level to say that $b_0$ is greater than zero.

In the context of the market regression model, the $b_0$ has an important meaning. If we find that $b_0$ is significantly greater than zero, then we can say that the stock is systematically offering positive returns over the market. In Finance the $b_0$ coefficient is called **Alpha of Jensen**, and it is supposed to always be zero or NOT significantly different than zero according to the **market efficiency hypothesis**.

For $b_1$ the same process is performed:

H0: The mean of $b_1$ = 0\
Ha: The mean of $b_1$ \<\> 0 (Our hypothesis)

In this case, **the variable of study for the hypothesis test is the** $b_0$ coefficient.

Then, we calculate the t-Statistic as follows:

$$
t =\frac{(b_1 - 0)}{SE(b_1)}
$$

$SE(b_1$)\$ is the standard error of $b_1$, which is its estimated standard deviation.

Then, we follow the same logic as it is explained above to make a conclusion about $b_1$.

In the context of market regression model, $b_1$ not only measures the linear relationship between the stock return and the market return; $b_1$ is a measure of systematic market risk of the stock. If the p-value($b_1$)\<0.05 and $b_1$\>0, then we can say that the stock return is positively and significantly related to market return.

Another interesting hypothesis test for $b_1$ is the examine whether $b_1$ is significantly greater or less than 1 (not zero). If $b_1$ is significantly \> 1, then we can say that the stock is significantly riskier than the market. Unfortunately, this hypothesis is NOT tested in the traditional output of the regression model. We need to calculate the corresponding t-Statistic for this test manually.

## The 95% confidence interval for each coefficient

Besides the standard error, t-Statistic and p-value, the 95% confidence interval (C.I.) is also calculated for each beta coefficient. The 95% C.I. has a minimum and maximum possible value. The 95% C.I. illustrates how much the beta coefficient can move 95% of the time.

An approximate way to estimate the minimum and maximum of this 95% C.I. is just by subtracting and adding 2 standard errors to the beta coefficient. For example, an approximate 95% C.I. for $b_0$ can be estimated as:

$$
95\%C.I.(b_0)=[b_0 - 2(SE(b_0) .. b_0 + 2(SE(b_0)]
$$ 

The exact critical value for the 95% is not 2, it depends on the \# of observations, but it can go from 1.8 to 2.1. The exact values of the 95%C.I. are automatically calculated when we run the regression model.

The 95% C.I. of a beta coefficient tells us the possible movement of the beta according to its standard error.

We can use the 95% C.I. instead of the t-Statistic or p-value to make the same conclusion for the hypothesis test of the coefficient. If the 95%C.I. does NOT contain the zero, then it means that the beta coefficient is significantly different than zero. An advantage of the 95%C.I. is that we could quickly test the hypothesis that $b_1$\>1 to check whether a stock is significantly riskier; if the 1 is not included in the 95% C.I. and $b_1>1$, then we can say that the stock is significantly riskier than the market at the 95% confidence level.

# CHALLENGE

Re-run the Market regression model for Alfa and respond to the following questions:

- How much the variability of the returns of Alfa is explained by the variability of the returns of the Market? 

- Is Alfa (on average) riskier than the Market? Why yes or why not? 
Hint: you have to look at the beta1 coefficient

- Is Alfa significantly riskier than the Market? Why yes or why not? 
Hint: you have to look at the beta1 coefficient, its standard error and/or its 95% confidence interval

- Is Alfa offering significantly excess returns over the market? why yes or why not? 
Hint: you have to look at the beta0 coefficient, its pvalu and/or its 95% confidence interval

# References
